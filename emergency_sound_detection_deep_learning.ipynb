{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbREcDwgU4-L",
        "outputId": "de6d8c32-5655-4a80-8fec-af7b9def1960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOJABJq-cPjJ",
        "outputId": "cea82c37-0d55-4df6-d32c-e9a33456210d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: audiomentations in /usr/local/lib/python3.12/dist-packages (0.43.1)\n",
            "Requirement already satisfied: numpy<3,>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (2.0.2)\n",
            "Requirement already satisfied: numpy-minmax<1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.5.0)\n",
            "Requirement already satisfied: numpy-rms<1,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.6.0)\n",
            "Requirement already satisfied: librosa!=0.10.0,<0.12.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.11.0)\n",
            "Requirement already satisfied: python-stretch<1,>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.3.1)\n",
            "Requirement already satisfied: scipy<2,>=1.4 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (1.16.2)\n",
            "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from audiomentations) (0.5.0.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.8.2)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install audiomentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzneFfrvVML-"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = r\"d:\\DL\\Gunshot_Detection_Project\\emergency sound detection\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTJXN2saVMU0",
        "outputId": "28e05aca-d8dd-4bfb-ecf5-c29993598cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Explosion dataset', 'Fire', 'GunShots', 'scream', 'non_scream']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir(DATASET_PATH))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1LUSAU7VMXF",
        "outputId": "a0510e85-afd2-4975-88c5-1b3b5b7be7c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Explosion dataset: 1294 files\n",
            "Fire: 289 files\n",
            "GunShots: 901 files\n",
            "scream: 2445 files\n",
            "non_scream: 596 files\n"
          ]
        }
      ],
      "source": [
        "for label in os.listdir(DATASET_PATH):\n",
        "    folder_path = os.path.join(DATASET_PATH, label)\n",
        "    if os.path.isdir(folder_path):\n",
        "        num_files = len([f for f in os.listdir(folder_path) if f.endswith(('.wav', '.mp3', '.flac', '.ogg'))])\n",
        "        print(f\"{label}: {num_files} files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDf0KS7PZAUM",
        "outputId": "165d6f8e-e907-4b8e-fd9c-06f7502a3e3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected classes: ['Explosion dataset', 'Fire', 'GunShots', 'non_scream', 'scream']\n",
            "\n",
            "Sample counts per class:\n",
            " - Explosion dataset: 1294\n",
            " - Fire: 289\n",
            " - GunShots: 901\n",
            " - non_scream: 596\n",
            " - scream: 2445\n"
          ]
        }
      ],
      "source": [
        "# Verify folder structure and count files per class\n",
        "import os\n",
        "valid_ext = ('.wav', '.mp3', '.flac', '.ogg', '.m4a')\n",
        "\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    raise FileNotFoundError(f\"Dataset path not found: {DATASET_PATH}\")\n",
        "\n",
        "classes = [d for d in sorted(os.listdir(DATASET_PATH)) if os.path.isdir(os.path.join(DATASET_PATH, d))]\n",
        "print(\"Detected classes:\", classes)\n",
        "\n",
        "counts = {}\n",
        "for c in classes:\n",
        "    p = os.path.join(DATASET_PATH, c)\n",
        "    files = [f for f in os.listdir(p) if f.lower().endswith(valid_ext)]\n",
        "    counts[c] = len(files)\n",
        "\n",
        "print(\"\\nSample counts per class:\")\n",
        "for k,v in counts.items():\n",
        "    print(f\" - {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYmAffLcVMag",
        "outputId": "2d932e75-dcbe-4809-a255-5c530fa766bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mixed precision enabled: <DTypePolicy \"mixed_float16\">\n"
          ]
        }
      ],
      "source": [
        "# Imports and parameters\n",
        "import os, random\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch, Shift\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Parameters\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 3\n",
        "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
        "N_MELS = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Use mixed precision if GPU supports it (speeds up training)\n",
        "try:\n",
        "    from tensorflow.keras import mixed_precision\n",
        "    policy = mixed_precision.Policy('mixed_float16')\n",
        "    mixed_precision.set_global_policy(policy)\n",
        "    print('Mixed precision enabled:', mixed_precision.global_policy())\n",
        "except Exception as e:\n",
        "    print('Mixed precision not enabled or not available:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRVATalCYl78"
      },
      "outputs": [],
      "source": [
        "# Augmentation pipeline (used only to augment smaller classes)\n",
        "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch, Shift\n",
        "\n",
        "augment = Compose([\n",
        "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.4),\n",
        "    PitchShift(min_semitones=-3, max_semitones=3, p=0.5),\n",
        "    TimeStretch(min_rate=0.9, max_rate=1.1, p=0.4),\n",
        "    Shift(p=0.5)  # ✅ updated syntax (no min/max_fraction)\n",
        "])\n",
        "\n",
        "\n",
        "def load_audio(file_path, sr=SAMPLE_RATE, duration=DURATION):\n",
        "    y, _sr = librosa.load(file_path, sr=sr, duration=duration)\n",
        "    if len(y) < sr*duration:\n",
        "        y = np.pad(y, (0, sr*duration - len(y)))\n",
        "    else:\n",
        "        y = y[:sr*duration]\n",
        "    return y\n",
        "\n",
        "def extract_mel(y, sr=SAMPLE_RATE, n_mels=N_MELS):\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=sr//2)\n",
        "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "    # Normalize to zero mean & unit variance per-sample\n",
        "    mel_db = (mel_db - np.mean(mel_db)) / (np.std(mel_db) + 1e-9)\n",
        "    return mel_db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwNjx6okY4hX",
        "outputId": "d8458419-2518-4037-8c83-773819e6ea9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target samples per class: 2445\n",
            "Processing class 'Explosion dataset' (1294 original samples)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 49/1294 [01:00<03:48,  5.44it/s]"
          ]
        }
      ],
      "source": [
        "# Build balanced dataset by augmenting only smaller classes\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# current file lists and counts\n",
        "class_files = {}\n",
        "for idx, c in enumerate(classes):\n",
        "    p = os.path.join(DATASET_PATH, c)\n",
        "    files = [os.path.join(p, f) for f in os.listdir(p) if f.lower().endswith(valid_ext)]\n",
        "    class_files[c] = files\n",
        "\n",
        "max_count = max(len(v) for v in class_files.values())\n",
        "print('Target samples per class:', max_count)\n",
        "\n",
        "for idx, c in enumerate(classes):\n",
        "    files = class_files[c]\n",
        "    print(f\"Processing class '{c}' ({len(files)} original samples)...\")\n",
        "    # add original samples\n",
        "    for fpath in tqdm(files):\n",
        "        y = load_audio(fpath)\n",
        "        mel = extract_mel(y)\n",
        "        data.append(mel)\n",
        "        labels.append(idx)\n",
        "    # augment until we reach max_count\n",
        "    while sum(1 for lab in labels if lab==idx) < max_count:\n",
        "        src = random.choice(files)\n",
        "        y = load_audio(src)\n",
        "        y_aug = augment(samples=y, sample_rate=SAMPLE_RATE)\n",
        "        mel = extract_mel(y_aug)\n",
        "        data.append(mel)\n",
        "        labels.append(idx)\n",
        "\n",
        "# Convert to arrays\n",
        "X = np.array(data)[..., np.newaxis].astype('float32')  # shape: (N, n_mels, frames, 1)\n",
        "y = to_categorical(np.array(labels), num_classes=len(classes))\n",
        "print('\\nFinal dataset shape:', X.shape, y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N__3edUMZKmu"
      },
      "outputs": [],
      "source": [
        "# Train-test split (stratified) and class weights\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
        "print('Train:', X_train.shape, 'Test:', X_test.shape)\n",
        "\n",
        "# compute class weights for optional use\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "y_integers = np.argmax(y_train, axis=1)\n",
        "class_weights_values = compute_class_weight('balanced', classes=np.unique(y_integers), y=y_integers)\n",
        "class_weights = dict(enumerate(class_weights_values))\n",
        "print('Class weights:', class_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tSJFo3QZNGK"
      },
      "outputs": [],
      "source": [
        "# Model: CNN + Bidirectional GRU\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_model(input_shape, num_classes):\n",
        "    inp = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(inp)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2,2))(x)\n",
        "\n",
        "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2,2))(x)\n",
        "\n",
        "    x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2,2))(x)\n",
        "\n",
        "    # prepare for RNN: collapse freq axis, keep time frames\n",
        "    # input shape: (n_mels, frames, 1) -> we permute to (frames, features)\n",
        "    shape = tf.shape(x)\n",
        "    x = layers.Permute((2,1,3))(x)  # now (batch, frames, n_mels/?, channels)\n",
        "    b, t, f, ch = x.shape.as_list()\n",
        "    x = layers.Reshape((tf.shape(x)[1], -1))(x)  # (batch, frames, features)\n",
        "\n",
        "    x = layers.Bidirectional(layers.GRU(128, return_sequences=False))(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # dtype float32 for stable output\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    return model\n",
        "\n",
        "input_shape = X_train.shape[1:]  # (n_mels, frames, 1)\n",
        "model = build_model(input_shape, num_classes=len(classes))\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLG2rMSeZO9m"
      },
      "outputs": [],
      "source": [
        "# Callbacks and training\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/emergency_sound_detection/best_model.h5\"\n",
        "callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1),\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_w5fAIyZRDZ"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.title('Accuracy'); plt.legend(); plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.title('Loss'); plt.legend(); plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Load best model (if saved) and evaluate\n",
        "from tensorflow.keras.models import load_model\n",
        "best = checkpoint_path\n",
        "if os.path.exists(best):\n",
        "    model = load_model(best)\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqgGny3eZTDR"
      },
      "outputs": [],
      "source": [
        "# Predictions, classification report and confusion matrix\n",
        "import numpy as np\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_true, y_pred_classes, target_names=classes))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZhmkgVZZVcG"
      },
      "outputs": [],
      "source": [
        "# Save final model to Drive\n",
        "final_path = \"/content/drive/MyDrive/emergency_sound_detection/final_model.h5\"\n",
        "model.save(final_path)\n",
        "print(\"Saved final model to:\", final_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzXQXGY0ZXOV"
      },
      "outputs": [],
      "source": [
        "# Inference on an uploaded file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    path = fn\n",
        "    print(\"Uploaded:\", path)\n",
        "    y = load_audio(path)\n",
        "    mel = extract_mel(y)\n",
        "    mel_input = mel[np.newaxis,...,np.newaxis].astype('float32')\n",
        "    preds = model.predict(mel_input)\n",
        "    idx = preds.argmax()\n",
        "    print(f\"Predicted: {classes[idx]} (confidence {preds[0,idx]*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG3oHke-Zbsy"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# 🎤 REAL-TIME AUDIO PREDICTION WITH MEL SPECTROGRAM\n",
        "# =====================================\n",
        "import io\n",
        "import numpy as np\n",
        "import sounddevice as sd\n",
        "import scipy.io.wavfile as wav\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# If model not already in memory:\n",
        "# model = load_model(\"/content/drive/MyDrive/emergency_sound_detection/audio_multiclass_cnn_gru_model.h5\")\n",
        "\n",
        "DURATION = 3   # seconds\n",
        "SR = 22050     # sample rate\n",
        "\n",
        "print(\"🎙️ Recording... Speak or make a sound now!\")\n",
        "recording = sd.rec(int(DURATION * SR), samplerate=SR, channels=1, dtype='float32')\n",
        "sd.wait()\n",
        "print(\"✅ Recording complete!\")\n",
        "\n",
        "# Save and playback\n",
        "wav.write(\"realtime_input.wav\", SR, recording)\n",
        "ipd.display(ipd.Audio(\"realtime_input.wav\"))\n",
        "\n",
        "# --- Feature Extraction Function ---\n",
        "def extract_mel(y, sr=SR, n_mels=128):\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
        "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "    return mel_db\n",
        "\n",
        "# Load & preprocess audio\n",
        "y, sr = librosa.load(\"realtime_input.wav\", sr=SR)\n",
        "mel = extract_mel(y)\n",
        "\n",
        "# --- Display Mel Spectrogram ---\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(mel, sr=sr, x_axis='time', y_axis='mel', cmap='magma')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title(\"🎛️ Mel Spectrogram (Real-Time Input)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Model Input Prep ---\n",
        "mel = mel / 80.0 + 1.0  # normalize\n",
        "mel_input = mel[np.newaxis, ..., np.newaxis].astype('float32')\n",
        "\n",
        "# --- Prediction ---\n",
        "preds = model.predict(mel_input)\n",
        "idx = preds.argmax()\n",
        "confidence = preds[0, idx] * 100\n",
        "\n",
        "print(f\"\\n🎯 Predicted Class: **{classes[idx]}**\")\n",
        "print(f\"🤖 Confidence: {confidence:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2e365CcZp7h"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import random\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ✅ Correct dataset path\n",
        "DATASET_PATH = \"/content/drive/MyDrive/emergency sound detection\"\n",
        "SAMPLE_RATE = 22050\n",
        "\n",
        "# Class labels (same as your folder names)\n",
        "LABELS = [\"scream\", \"non_scream\", \"GunShots\", \"Explosion dataset\", \"Fire\"]\n",
        "\n",
        "# Function to extract mel spectrogram\n",
        "def extract_mel(y, sr=22050, n_mels=128):\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
        "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "    return mel_db\n",
        "\n",
        "# Loop through each class and display one random Mel spectrogram\n",
        "for label in LABELS:\n",
        "    class_path = os.path.join(DATASET_PATH, label)\n",
        "    files = [f for f in os.listdir(class_path) if f.lower().endswith(('.wav', '.mp3'))]\n",
        "    if not files:\n",
        "        print(f\"⚠️ No audio files found in {label}\")\n",
        "        continue\n",
        "\n",
        "    # Pick a random file\n",
        "    file_path = os.path.join(class_path, random.choice(files))\n",
        "    y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "\n",
        "    # Extract mel spectrogram\n",
        "    mel_spec = extract_mel(y, sr)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(mel_spec, sr=sr, x_axis='time', y_axis='mel', cmap='magma')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(f\"🎧 Mel Spectrogram - {label}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === Load model ===\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(\"audio_multiclass_cnn_gru_model.h5\")\n",
        "\n",
        "# === Feature extraction ===\n",
        "def extract_mel_spectrogram(file_path, sr=22050, n_mels=130, max_len=128):\n",
        "    y, sr = librosa.load(file_path, sr=sr)\n",
        "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
        "    mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    mel_db = mel_db.T  # shape: (time, freq)\n",
        "    \n",
        "    # Pad or trim to match model input (128, 130)\n",
        "    if mel_db.shape[0] < max_len:\n",
        "        pad_width = max_len - mel_db.shape[0]\n",
        "        mel_db = np.pad(mel_db, ((0, pad_width), (0, 0)), mode='constant')\n",
        "    else:\n",
        "        mel_db = mel_db[:max_len, :]\n",
        "    \n",
        "    return mel_db\n",
        "\n",
        "# === Interpretability functions ===\n",
        "def compute_saliency(model, mel_features):\n",
        "    input_tensor = tf.convert_to_tensor(mel_features[np.newaxis, ..., np.newaxis], dtype=tf.float32)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(input_tensor)\n",
        "        preds = model(input_tensor)\n",
        "        top_class = tf.argmax(preds[0])\n",
        "        loss = preds[0, top_class]\n",
        "    grads = tape.gradient(loss, input_tensor)\n",
        "    saliency = np.abs(grads[0, ..., 0].numpy())\n",
        "    return saliency\n",
        "\n",
        "def smoothgrad(model, mel_features, n_samples=30, noise_level=0.1):\n",
        "    grads_sum = np.zeros_like(mel_features)\n",
        "    for _ in range(n_samples):\n",
        "        noise = np.random.normal(0, noise_level, mel_features.shape)\n",
        "        noisy_input = mel_features + noise\n",
        "        grads_sum += compute_saliency(model, noisy_input)\n",
        "    return grads_sum / n_samples\n",
        "\n",
        "def occlusion_sensitivity(model, mel_features, patch_size=(8, 8)):\n",
        "    base_pred = model.predict(mel_features[np.newaxis, ..., np.newaxis])[0]\n",
        "    sensitivity = np.zeros_like(mel_features)\n",
        "    for i in range(0, mel_features.shape[0], patch_size[0]):\n",
        "        for j in range(0, mel_features.shape[1], patch_size[1]):\n",
        "            occluded = mel_features.copy()\n",
        "            occluded[i:i+patch_size[0], j:j+patch_size[1]] = 0\n",
        "            pred = model.predict(occluded[np.newaxis, ..., np.newaxis])[0]\n",
        "            diff = np.abs(base_pred - pred).sum()\n",
        "            sensitivity[i:i+patch_size[0], j:j+patch_size[1]] = diff\n",
        "    return sensitivity\n",
        "\n",
        "# === Example for each class ===\n",
        "classes = [\"Explosion\", \"Fire\", \"Gunshots\", \"Non-Scream\", \"Scream\"]\n",
        "test_files = {\n",
        "    \"Explosion\": \"C:/Users/Prathima B A/Downloads/Scream_detection/Explosion dataset/timebomb-74798.mp3\",\n",
        "    \"Fire\": \"C:/Users/Prathima B A/Downloads/Scream_detection/Fire/videoplayback (1)_62.wav\",\n",
        "    \"Gunshots\":\"C:/Users/Prathima B A/Downloads/Scream_detection/GunShots/8 (5).wav\",\n",
        "    \"Non-Scream\": \"C:/Users/Prathima B A/Downloads/Scream_detection/non_scream/z_mf1ceM8jc_out.wav\",\n",
        "    \"Scream\": \"C:/Users/Prathima B A/Downloads/Scream_detection/scream/zkWoni28n64_out.wav\"\n",
        "}\n",
        "\n",
        "for label, file_path in test_files.items():\n",
        "    print(f\"\\n🎧 {label}: {file_path}\")\n",
        "    mel_features = extract_mel_spectrogram(file_path)\n",
        "    \n",
        "    sal_map = compute_saliency(model, mel_features)\n",
        "    sg_map = smoothgrad(model, mel_features)\n",
        "    occ_map = occlusion_sensitivity(model, mel_features)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(sal_map.T, aspect='auto', origin='lower', cmap='viridis')\n",
        "    plt.title('Saliency')\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(sg_map.T, aspect='auto', origin='lower', cmap='inferno')\n",
        "    plt.title('SmoothGrad')\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(occ_map.T, aspect='auto', origin='lower', cmap='magma')\n",
        "    plt.title('Occlusion')\n",
        "    plt.suptitle(f\"Class: {label}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
